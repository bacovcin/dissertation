\chapter{Introduction}
\label{ch:introduction}

% Larger picture situation
This dissertation addresses three larger questions: (a) what is the distribution of labour between the syntactic and morphological components of the grammar, (b) what aspects of syntax are universal/language particular, and (c) what methods can/should be used to address morphosyntactic problems. The first question bears on the architecture of the grammar, namely which surface properties are driven by the presence/absence/position of syntactic atoms and which properties are driven by the phonological (and semantic) realisation of those atoms. The answer to this question ideally reduces surface complexity to the interaction of simple independently necessary syntactic and morphological operations.

The second question (what aspects of syntax are universal/language particular) has direct implications for Plato's Problem, namely how do children acquire language as quickly as they do. Assuming that only language particular material needs to be acquired, the more universal properties that can be ascribed to human language, the easier it is to solve Plato's Problem \citep{Chomsky.1993}. The specific aspect of this question addressed here is the tension between argument structure and movement operations; different word orders could arise either by (a) being base generated in each position or (b) created by moving arguments from a previous (moved or base generated) position. This dissertation argues that there is no variation in base generation (the strong version of the UTAH hypothesis, \citealt{Baker.1988}) and that syntactic variation comes from differences in movement operations, morphological realisations, and associations of particular semantic concepts with the universally provided base constructions.

The answer to the final question (what methods are necessary to address morphosyntactic problems) depends on the nature of the problems being considered. The first question (on addressing the derivation of surface complexity) requires studying situations that involve some degree of surface complexity. However, in most cases, it is impossible to tease apart closely related solutions by looking at a single construction in a single language. The need to consider data from multiple sources is even more acute in the case of the second question. In order to plausibly argue for universality, it is necessary to demonstrate that the universal analysis has empirical coverage over a variety of distinct surface realisations. This dissertation solves this problem in two ways: (a) by using data from languages throughout the Germanic family and (b) bringing in qualitative and quantitative analysis of language change.

Typological study of closely related languages permits necessary comparisons. Often one language cannot provide the necessary data to support any given analysis (the crucial data is ambiguous or the necessary constructions do not exist for reasons irrelevant to the current theoretical question). However, a closely related language often provides the needed data, while being similar enough to the first language that we can be confident that the relevant theoretical implications are the same. The Germanic language family has the advantage of containing a number of well studied languages (including English, which has received the largest share of linguistic inquiry of any language), which provide a large amount of morphosyntactic variation (e.g., presence/absence of complex inflectional morphology and OV vs. VO word order). This variation, however, occurs within the framework of familial similarity that comes from all the languages being derived from a common ancestor. Variation within a broader framework of similarity helps reveal true comparisons between related elements, which might otherwise be obscured by irrelevant differences between the languages in question.

Another reason to study Germanic languages is the ability to do large scale quantitative diachronic research. Traditional syntactic inquiry has relied on the use of acceptability judgements to probe grammatical structure. However, it has been noted since the beginnings of the generative program that acceptability judgements are not a perfect probe for grammatical structure (see \citealt{Schutze.1996} for a discussion of the history of this issue). While acceptable sentences (when the judgement was given after long deliberation) are presumably all grammatical, ungrammaticality is only one of a number of factors that can contribute to unacceptability. Generative linguistics has developed a number of techniques for trying to overcome this issue (e.g., the use of multiple different lexicalisation and providing explicit contexts to alleviate pragmatic issues), quantitative corpus data provides an independent source of information about grammatical structure (\citealt{Kroch.1989,Kroch.1994} and others working in this programme). We can be more confident in conclusions that are supported by both sources of information (since they tap into different aspects of the grammatical processes and thus the probability that both would coincidently point to the same conclusion is much smaller than the probability that either would individually).

The case study that I have used to address the first two questions is the analysis of recipients in Germanic ditransitives. Ditransitive clauses provide the necessary surface complexity to be able to study how different grammatical components interact to produce that complexity. By constraining my focus to a particular semantic feature (recipients), I legitimate cross-linguistic comparisons in looking for universals. Assuming that all languages have the expressive capacity to capture any semantic notion and by  holding semantics constant, we can study  what morphosyntactic correlates of the semantics are universal and which are subject to linguistic variation.

The main theoretical claim of the dissertation is that recipients are universally base generated as dative PPs in the specifier of an applicative phrase (henceforth the dative PP + applicative analysis). As the dissertation progresses, a number of ancillary morphological and syntactic operations will be argued for to generate the surface complexity seen in Germanic. While none of the components (main or ancillary) are original, this dissertation provides a unique combination of previous theoretical proposals. Also, while cross-linguistic study of Germanic ditransitives have been employed previously (\citealt{Falk.1990}, \citealt{Sprouse.1995}, and \citealt{Holmberg.1995}, among others) this dissertation is the first complete survey of Germanic ditransitive data from all (major) extant Germanic languages. As such, all natural language examples are collected by language in an appendix for ease of reference.

The dissertation has the following structure. Chapter \ref{ch:theoryback} presents the theoretical background for the dissertation. This chapter focuses on presenting the theoretical claims in an abstract way independent from the inherent messiness of any natural language examples. Each component of the main claim is explicated. The focus is on the claims relevant to the base generation of recipients; issues related to morphosyntactic operations are discussed in the context of the data that motivates positing them. Where appropriate, my theory is situated among other live possibilities from the literature. When multiple theories are presented, a brief discussion of the differences in empirical predictions are presented. These empirical predictions are tested against natural language data in the following chapters.

Chapter \ref{ch:active} presents data concerning active ditransitive constructions in Germanic. The focus is providing the data that demonstrates the empirical coverage of the dative PP + applicative analysis. Variation in the marking of recipients between unmarked, marked with synthetic dative case, and introduced by overt preposition (e.g., English \textit{to}) is explained by reference to allomorphy (i.e., the same operation that explains the variation in plural marking between dogs, sheep, children and men). Three syntactic operations are introduced: (a) VP-internal scrambling, (b) pronoun cliticisation, and (c) P-incorporation. The focus is on VP-internal scrambling and pronoun cliticisation, since P-incorporation is one of the major focuses of the next chapter.

Chapter \ref{ch:passive} presents data concerning passive ditransitive constructions in Germanic. I explore different distributions of subject properties (namely raising to spec-TP and receiving nominative case) over the recipient and theme. P-incorporation is used to explain dative-to-nominative raising, while unincorporated dative Ps provide a fertile study of passive locality. Across (and sometimes within the same language) dative Ps range from being valid targets of passivisation through being invisible for locality to being defective interveners.

Chapter \ref{ch:diachron} returns to the question of linguistic evidence and presents two case studies in using quantitative data about language use to support grammatical arguments. Building on the arguments from the previous two chapters, this chapter discusses changes in recipient marking and recipient passivisation in the history of the English language. I show how the quantitative data provides independent support for the analysis previously suggested, and how the data can independently give information about the nature of language change.

The final chapter summarises the support for the dative PP + applicative analysis. The chapter then returns to the larger questions introduced at the beginning of this chapter and argues for what (partial) answers the dative PP + applicative analysis provides. Finally, some further implications and broader predictions are provided. 
%\bibliography{diss}

NEEDS INTEGRATION

Theoretical linguistics has a problem (common to the social sciences) of finding empirical validations for theoretical claims. Building on the work starting during the cognitive revolution in the 50s and 60s, the goal of generative linguistics has been to study the linguistic competence of speakers, which consists of the language specific information that is needed to use a language natively \citep{Chomsky.1981,Chomsky.1986}. As will be shown below (in particular when looking at passivisation), this linguistic competence can be separated into grammatical and non-grammatical competences. This distinction between grammatical and non-grammatical competences depends on a specific notion of grammar.
``
A grammar can be thought of as a list of the possible sound--meaning pairs in a language. Since at least \cite{Chomsky.1957}, it has been recognised that this list is infinitely long for any natural language (because of the recursive nature of natural language). The generative grammar program has endeavoured to describe a set of finite rules which are capable of generating the correct sound--meaning pairs. Often, an even simpler goal is attempted, namely to separate strings (chunks of sound) into two sets: (a) the strings that have at least one meaning associated with them (grammatical strings) and (b) the strings that have no meaning associated with them (ungrammatical strings). Note that these meanings do not need to plausibly arise in actual discourse; all that is necessary for a string to be grammatical is that it have \textbf{some} meaning associated with it.\footnote{The classic example from Chomsky's work is ``Colourless green ideas sleep furiously'', which certainly has no real world referent, but is grammatical and has a meaning associated with it (simply a nonsensical meaning). Given that contradictions are stateable in natural languages, whatever our definition of meaning is for grammaticality, it must be able to include nonsensical meanings. That these meanings are truth conditionally equivalent, but yet are felt to be distinct for speakers (e.g., ``Both A and not A'' and ``X equals 1 and not 1'' are both contradictions, but have different meanings), suggests that natural language meaning is fundamentally intensional rather than extensional.}

Given the rampant ambiguity in natural language, the grammar of natural languages often associates multiple strings with the same meaning (and multiple meanings with the same string). Since the purpose of the grammar is to simply list whether a string is associated with an utterance, it cannot help a speaker decide which string to use in production from among the set of strings compatible with the meaning they are trying to express. This problem of knowing which of the options produced by the grammar to use in any particular circumstance is an equally important part of any native speakers linguistic competence. These choices are often impacted by language specific implementations of general social or psychological factors (see \cite{Bresnan.2007,Bresnan.2010,Zeevat.2014} and \cite{Tamminga.2016} for a discussion of these issues and their relationship to the grammar). These choices can be formalised as representing probability distributions over the forms provided for by the grammar.

Given that such probability distributions need to exist (in order for speakers to use language), it is worthwhile to discuss what properties they might have. These choices often depend on specific properties of the strings in questions (e.g., on the prosodic heaviness of certain arguments for determining the likelihood of Heavy NP shift). Therefore, the same logic that motivated adopting generative approaches to grammar (i.e., the impossibility of simply listing the grammatical/ungrammatical pairings) applies here. It would be impossible to simply memorise the relevant probability distributions, since they apply (and are affected by properties of) an infinite number of strings. Thus, a generative mechanism for producing probability distributions for any given set of grammatical alternatives is necessary.

Unfortunately, it has been known since the beginning of the generative grammar enterprise that there is no direct evidence of linguistic competence (see \cite{Schutze.1996} for a discussion of early claims about this issue), which is typical of knowledge and psychological constructs. Instead, it has been necessary to deduce the nature of the linguistic knowledge by studying its effects on language performance (see \citealt{Stroud.2012,Phillips.2013, Phillips.2013b, Phillips.2013c} for an arguments that acceptability judgements are fundamentally performative).

One of the most prominent types of linguistic performance to be used in theoretical linguistics is the acceptability judgement. These judgements reflect a native speakers sensation of naturalness/unnaturalness upon encountering a particular linguistic utterance. These sensations have a cognitive reality similar to that of pain sensations \citep{Schutze.2014}. A major advantage to the acceptability judgement is that even utterances that would never occur in natural production (due to the combination of factors each of which is extremely infrequent) can still be studied. However, as mentioned in the first chapter, grammaticality is only one aspect that contributes to the sensation of naturalness; other factors (such as pragmatic concerns) can often render a perfectly grammatical utterance unnatural (e.g., because there is a more concise grammatical way of conveying the same information). Trained linguists (and ideal native language informants) are able to minimise contextual factors that impact naturalness by attempting to evaluate the utterance in a number of hypothetical linguistic contexts, but these techniques cannot rescue a grammatical utterance that is ruled out because of context independent problems inherent to the utterance itself (e.g., prosodic ill-formedness). These non-grammatical problems often have a gradual impact on acceptability, reflecting a gradient notion of pragmatic infelicity or psychological complexity \citep{Bresnan.2007,Bresnan.2010,Schutze.2014}.

Quantitative studies of language performance are useful for isolating these gradient factors, so that they can be factored out when studying grammaticality. Since corpora (ideally) provide multiple instances of the relevant features in a variety of pragmatic contexts, the gradient effects of non-grammatical factors can be investigated for the observed contexts and statistically extrapolated to unobserved contexts. In addition, corpora provide a means of studying diachronic processes that cannot be studied using traditional acceptability judgements, since the earlier speakers in the diachronic process are unavailable for consultation. Assuming that language change cannot radically alter the underlying grammar (since the speakers of the new variety must participate in a speech community with speakers of the old variety), it is possible to provide independent evidence concerning the internal structure of the relevant grammatical processes.


