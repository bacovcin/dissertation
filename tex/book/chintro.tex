\chapter{Introduction}
\label{ch:introduction}

% Larger picture situation
This dissertation addresses three larger questions: (a) what is the distribution of labour between the syntactic and morphological components of the grammar, (b) what aspects of syntax are universal/language particular, and (c) what methods can/should be used to address morphosyntactic problems. The first question bears on the architecture of the grammar, namely which surface properties are driven by the presence/absence/position of syntactic atoms and which properties are driven by the phonological (and semantic) realisation of those atoms. The answer to this question ideally reduces surface complexity to the interaction of simple independently necessary syntactic and morphological operations.

Part of this question also relies on determining what aspects of language can be attributed to the grammar, and what should be attributed to extra-grammatical factors. A grammar can be thought of as a list of the possible sound--meaning pairs in a language. Since at least \cite{Chomsky.1957}, it has been recognised that this list would be infinitely long for any natural language (because of the recursive nature of natural language). The generative grammar program has endeavoured to describe a set of finite rules which are capable of generating the correct sound--meaning pairs. Often, an even simpler goal is attempted, namely to separate strings (chunks of sound) into two sets: (a) the strings that have at least one meaning associated with them (grammatical strings) and (b) the strings that have no meaning associated with them (ungrammatical strings). Note that these meanings do not need to plausibly arise in actual discourse; all that is necessary for a string to be grammatical is that it have \textbf{some} meaning associated with it.\footnote{The classic example from Chomsky's work is ``Colourless green ideas sleep furiously'', which certainly has no real world referent, but is grammatical and has a meaning associated with it (simply a nonsensical meaning). Given that contradictions are stateable in natural languages, whatever our definition of meaning is for grammaticality, it must be able to include nonsensical meanings. That these meanings are truth conditionally equivalent, but yet are felt to be distinct for speakers (e.g., ``Both A and not A'' and ``X equals 1 and not 1'' are both contradictions, but have different meanings), suggests that natural language meaning is fundamentally intensional rather than extensional.}

The grammar of natural languages often associates multiple strings with the same meaning (and multiple meanings with the same string). Since the purpose of the grammar is to simply list whether a string is associated with an meaning, it cannot help a speaker decide which string to use in production from among the set of strings compatible with the meaning they are trying to express. This problem of knowing which of the options produced by the grammar to use in any particular circumstance is an equally important part of any native speakers linguistic competence. These choices are often impacted by language specific implementations of general social or psychological factors (see \cite{Bresnan.2007,Bresnan.2010,Zeevat.2014} and \cite{Tamminga.2016} for a discussion of these issues and their relationship to the grammar). These choices can be formalised as representing probability distributions over the forms provided for by the grammar.

Given that such probability distributions need to exist (in order for speakers to use language), it is worthwhile to discuss what properties they might have. These choices often depend on specific properties of the strings in questions (e.g., on the prosodic heaviness of certain arguments for determining the likelihood of Heavy NP shift). Therefore, the same logic that motivated adopting generative approaches to grammar (i.e., the impossibility of simply listing the grammatical/ungrammatical pairings) applies here. It would be impossible to simply memorise the relevant probability distributions, since they apply to (and are affected by properties of) an infinite number of strings. Thus, a generative mechanism for producing probability distributions for any given set of grammatical alternatives is necessary. While the existence of such a generative method for probabilities is referenced at various points in this text, a full fledged theory of generative probability would require another dissertation. For this text, the essential points are that: (a) there exists some non-grammatical component of linguistic competence responsible for determining the probability of particular utterances in cases of ambiguity and (b) as will be discussed in more detail below that this non-grammatical component has a role in all aspects of linguistic performance, including acceptability judgements.

The second question mentioned above (what aspects of syntax are universal/language particular) has direct implications for Plato's Problem, namely how do children acquire language as quickly as they do. Assuming that only material particular to the relevant language needs to be acquired, the more universal properties that can be ascribed to human language or related cognitive systems, the easier it is to solve Plato's Problem \citep{Chomsky.1993}. The specific aspect of this question addressed here is the tension between argument structure and movement operations; different word orders could arise either by (a) being base generated in each position or (b) created by moving arguments from a previous (moved or base generated) position. This dissertation argues that there is no variation in base generation (the strong version of the Uniformity of Theta Assignment Hypothesis, \citealt{Baker.1988}) and that syntactic variation comes from differences in movement operations, morphological realisations, and associations of particular semantic concepts with the universally provided base constructions.

The answer to the final question (what methods are necessary to address morphosyntactic problems) depends on the nature of the problems being considered. Theoretical linguistics has a problem (common to the social sciences) of finding empirical validations for theoretical claims. Building on the work starting during the cognitive revolution in the 50s and 60s, the goal of generative linguistics has been to study the linguistic competence of speakers, which consists of the language specific information that is needed to use a language natively \citep{Chomsky.1981,Chomsky.1986}. Unfortunately, it has been known since the beginning of the generative grammar enterprise that there is no direct evidence of linguistic competence (see \cite{Schutze.1996} for a discussion of early claims about this issue), which is typical of knowledge and psychological constructs. Instead, it has been necessary to deduce the nature of the linguistic knowledge by studying its effects on language performance.

One of the most prominent types of linguistic performance to be used in theoretical linguistics is the acceptability judgement (see \citealt{Stroud.2012,Phillips.2013, Phillips.2013b, Phillips.2013c} for an arguments that acceptability judgements are fundamentally performative). These judgements reflect a native speaker's sensation of naturalness/unnaturalness upon encountering a particular linguistic utterance in context. These sensations have a cognitive reality similar to that of pain sensations \citep{Schutze.2014}. A major advantage to the acceptability judgement is that even utterances that would never occur in natural production (due to a combination of contextual factors each of which is extremely infrequent) can still be studied. However, as mentioned above, grammaticality is only one aspect that contributes to the sensation of naturalness; other factors (such as pragmatic concerns) can often render a perfectly grammatical utterance unnatural (e.g., because there is a more concise grammatical way of conveying the same information). Trained linguists (and ideal native language informants) are able to minimise contextual factors that impact naturalness by attempting to evaluate the utterance in a number of hypothetical linguistic contexts, but these techniques cannot rescue a grammatical utterance that is ruled out because of context independent problems inherent to the utterance itself (e.g., cultural taboos rendering an utterance an unacceptable way of conveying some meaning). These non-grammatical problems often have a gradual impact on acceptability, reflecting a gradient notion of pragmatic infelicity or psychological complexity \citep{Bresnan.2007,Bresnan.2010,Schutze.2014}.

Quantitative studies of language performance are useful for isolating these extra-grammatical factors, so that they can be factored out when studying grammaticality. Since corpora (ideally) provide multiple instances of the relevant features in a variety of pragmatic contexts, the gradient effects of non-grammatical factors can be investigated for the observed contexts and statistically extrapolated to unobserved contexts. In addition, corpora provide a means of studying diachronic processes that cannot be studied using traditional acceptability judgements, since the earlier speakers in the diachronic process are unavailable for consultation. 


Returning to the overarching questions, the question on addressing the underlying sources of surface complexity requires studying situations that involve some degree of surface complexity. However, in most cases, it is impossible to tease apart closely related solutions by looking at a single construction in a single language. The need to consider data from multiple sources is even more acute in the case of the question of language particulars versus universals. In order to plausibly argue for universality, it is necessary to demonstrate that the universal analysis has empirical coverage over a variety of distinct surface realisations. This dissertation solves this problem in two ways: (a) by using data from languages throughout the Germanic family and (b) bringing in qualitative and quantitative analysis of language change.

Typological study of closely related languages permits necessary comparisons for investigating the predictions of a particular syntactic analysis. Often one language cannot provide the necessary data to support any given analysis (the crucial data is ambiguous or the necessary constructions do not exist for reasons irrelevant to the current theoretical question). However, a closely related language often provides the needed data, while being similar enough to the first language that we can be confident that the relevant theoretical implications are the same. The Germanic language family has the advantage of containing a number of well studied languages (including English, which has received the largest share of linguistic inquiry of any language), which encompass a large degree of morphosyntactic variation (e.g., presence/absence of complex inflectional morphology and OV vs. VO word order). This variation, however, occurs within the framework of familial similarity that comes from all the languages being derived from a common ancestor. Variation within a broader framework of similarity helps reveal true comparisons between related elements, which might otherwise be obscured by irrelevant differences between the languages in question.

Another reason to study Germanic languages is the ability to do large scale quantitative diachronic research. Diachronic investigations provide their own independent verification of linguistic theories parallel to the type of data found in synchronic typological investigations. Language change cannot radically alter underlying grammars, since the speakers of the new variety must participate in a speech community with speakers of the old variety. Instead, change must proceed via gradual alternations, which introduce small variation between otherwise identical grammars. This makes diachrony an ideal place for typological investigation; variation between stages of a language closely related in time provide the nearest example to controlled experimentation available using natural language data. In addition, patterns of co-occurrence between changes in surface forms can reveal underlying structures (\citet{Kroch.1989}; see Chapter \ref{ch:diachron} for further discussion on the use of quantitative diachronic investigation).

As discussed above, traditional syntactic inquiry has relied on the use of acceptability judgements to probe grammatical structure. However, it has been noted since the beginnings of the generative program that acceptability judgements are not a perfect probe for grammatical structure (see \citealt{Schutze.1996} for a discussion of the history of this issue). Acceptable sentences (when the judgement was given after long deliberation) are all grammatical\footnote{Barring grammaticality illusions, such as those that arise in cases of agreement attraction, e.g., ``The key to the cabinets are on the table.''}, but ungrammaticality is only one of a number of factors that can contribute to unacceptability. While generative linguistics has developed a number of techniques for trying to overcome this issue (e.g., the use of multiple different lexicalisation and providing explicit contexts to alleviate pragmatic issues), quantitative corpus data provides an independent source of information about grammatical structure (\citealt{Kroch.1989,Kroch.1994} and others working in this programme). We can be more confident in conclusions that are supported by both sources of information, since the odds that both production and comprehension would coincidentally support the same conclusion are lower than the odds that production or comprehension would support a conclusions on their own.

The case study that I have used to address the first two questions is the analysis of recipients in Germanic ditransitives. Ditransitive clauses provide the necessary surface complexity to be able to study how different grammatical components interact to produce that complexity. By constraining my focus to a particular semantic feature (recipients), I legitimise cross-linguistic comparisons in looking for universals. While all languages have the expressive capacity to capture any semantic notion (and thus ultimately share all semantic features), not all languages possess all morphosyntactic constructions. A comparison on semantic grounds is, thus, guaranteed to find a correlate construction in all languages, but a comparison on morphosyntactic grounds is likely to have gaps. By holding semantics constant, we can study which morphosyntactic correlates of the semantics are universal and which are subject to linguistic variation.

The main theoretical claim of the dissertation is that recipients are base generated as dative PPs in the specifier of an applicative phrase (henceforth the dative PP + applicative analysis). As the dissertation progresses, a number of ancillary morphological and syntactic operations will be proposed to generate the surface complexity seen in Germanic. While none of the components (main or ancillary) are original, this dissertation provides a unique combination of previous theoretical proposals. Also, while cross-linguistic study of Germanic ditransitives have been employed previously (\citealt{Falk.1990}, \citealt{Sprouse.1995}, and \citealt{Holmberg.1995}, among others) this dissertation is the first complete survey of Germanic ditransitive data from all modern standard Germanic languages and also includes some relevant dialect data. As such, all natural language examples as well as a list of references used as sources for data for each language are collected by language in Appendix B for ease of reference.

The dissertation has the following structure. Chapter \ref{ch:theoryback} presents the theoretical background for the dissertation. This chapter focuses on presenting the theoretical claims in an abstract way independent from the inherent messiness of any natural language examples. Each component of the main claim is explicated. Morphosyntactic operations necessary for deriving surface forms from the base generated structure are also introduced. Where appropriate, the theory argued for her is situated among other live possibilities from the literature. When multiple theories are presented, a brief discussion of the differences in empirical predictions is presented. These empirical predictions are tested against natural language data in the following chapters.

Chapter \ref{ch:active} presents data concerning active ditransitive constructions in Germanic. I prioritise providing the data that demonstrates the empirical coverage of the dative PP + applicative analysis. Variation in the marking of recipients between unmarked, marked with synthetic dative case, and introduced by overt preposition (e.g., English \textit{to}) is explained by reference to allomorphy (i.e., the same operation that explains the variation in plural marking between \textit{dogs}, \textit{sheep}, \textit{children} and \textit{men}). Evidence is provided to support the following three morphosyntactic operations: (a) VP-internal scrambling, (b) pronoun cliticisation, and (c) P-incorporation. 

Chapter \ref{ch:passive} presents data concerning passive ditransitive constructions in Germanic. I explore different distributions of subject properties (namely raising to spec-TP and receiving nominative case) over the recipient and theme. P-incorporation is used to explain dative-to-nominative raising, while unincorporated dative Ps provide a fertile study of passive locality. Across (and sometimes within the same language) dative Ps range from being valid targets of passivisation through being invisible for locality to being defective interveners.

Chapter \ref{ch:diachron} returns to the question of linguistic evidence and presents two case studies in using quantitative data about language use to support grammatical arguments. Building on the arguments from the previous two chapters, this chapter discusses changes in recipient marking and recipient passivisation in the history of the English language. I show how the quantitative data provides independent support for the analysis previously suggested, and how the historical data can give information about the nature of language change and linguistic competence.

The final chapter summarises the support for the dative PP + applicative analysis. The chapter then returns to the larger questions introduced at the beginning of this chapter and argues for what (partial) answers the dative PP + applicative analysis provides. Finally, some further implications and broader predictions are provided. 
%\bibliography{diss}
